correlation = cor(x, y, method=cor_method, use="complete.obs")
data.frame(xName, yName, assoc=correlation, type="correlation")
}else if(is_numeric(x) && is_nominal(y)){
# from https://stats.stackexchange.com/questions/119835/correlation-between-a-nominal-iv-and-a-continuous-dv-variable/124618#124618
r_squared = summary(lm(x ~ y))$r.squared
data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")
}else if(is_nominal(x) && is_numeric(y)){
r_squared = summary(lm(y ~x))$r.squared
data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")
}else {
warning(paste("unmatched column type combination: ", class(x), class(y)))
}
# finally add complete obs number and ratio to table
result %>% mutate(complete_obs_pairs=sum(!is.na(x) & !is.na(y)), complete_obs_ratio=complete_obs_pairs/length(x)) %>% rename(x=xName, y=yName)
}
# apply function to each variable combination
map2_df(df_comb$X1, df_comb$X2, f)
}
#correlation
df_res = mixed_assoc(seeds)
#secondary function
mixed_assoc = function(df, cor_method="spearman", adjust_cramersv_bias=TRUE){
df_comb = expand.grid(names(df), names(df),  stringsAsFactors = F) %>% set_names("X1", "X2")
is_nominal = function(x) class(x) %in% c("factor", "character")
# https://community.rstudio.com/t/why-is-purr-is-numeric-deprecated/3559
# https://github.com/r-lib/rlang/issues/781
is_numeric <- function(x) { is.integer(x) || is_double(x)}
f = function(xName,yName) {
x =  pull(df, xName)
y =  pull(df, yName)
result = if(is_nominal(x) && is_nominal(y)){
# use bias corrected cramersV as described in https://rdrr.io/cran/rcompanion/man/cramerV.html
cv = cramerV(as.character(x), as.character(y), bias.correct = adjust_cramersv_bias)
data.frame(xName, yName, assoc=cv, type="cramersV")
}else if(is_numeric(x) && is_numeric(y)){
correlation = cor(x, y, method=cor_method, use="complete.obs")
data.frame(xName, yName, assoc=correlation, type="correlation")
}else if(is_numeric(x) && is_nominal(y)){
# from https://stats.stackexchange.com/questions/119835/correlation-between-a-nominal-iv-and-a-continuous-dv-variable/124618#124618
r_squared = summary(lm(x ~ y))$r.squared
data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")
}else if(is_nominal(x) && is_numeric(y)){
r_squared = summary(lm(y ~x))$r.squared
data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")
}else {
warning(paste("unmatched column type combination: ", class(x), class(y)))
}
# finally add complete obs number and ratio to table
result %>% mutate(complete_obs_pairs=sum(!is.na(x) & !is.na(y)), complete_obs_ratio=complete_obs_pairs/length(x)) %>% rename_(x=xName, y=yName)
}
# apply function to each variable combination
map2_df(df_comb$X1, df_comb$X2, f)
}
#correlation
df_res = mixed_assoc(seeds)
# finally add complete obs number and ratio to table
result %>% mutate(complete_obs_pairs=sum(!is.na(x) & !is.na(y)), complete_obs_ratio=complete_obs_pairs/length(x)) %>% rename(x=xName, y=yName)
#secondary function
mixed_assoc = function(df, cor_method="spearman", adjust_cramersv_bias=TRUE){
df_comb = expand.grid(names(df), names(df),  stringsAsFactors = F) %>% set_names("X1", "X2")
is_nominal = function(x) class(x) %in% c("factor", "character")
# https://community.rstudio.com/t/why-is-purr-is-numeric-deprecated/3559
# https://github.com/r-lib/rlang/issues/781
is_numeric <- function(x) { is.integer(x) || is_double(x)}
f = function(xName,yName) {
x =  pull(df, xName)
y =  pull(df, yName)
result = if(is_nominal(x) && is_nominal(y)){
# use bias corrected cramersV as described in https://rdrr.io/cran/rcompanion/man/cramerV.html
cv = cramerV(as.character(x), as.character(y), bias.correct = adjust_cramersv_bias)
data.frame(xName, yName, assoc=cv, type="cramersV")
}else if(is_numeric(x) && is_numeric(y)){
correlation = cor(x, y, method=cor_method, use="complete.obs")
data.frame(xName, yName, assoc=correlation, type="correlation")
}else if(is_numeric(x) && is_nominal(y)){
# from https://stats.stackexchange.com/questions/119835/correlation-between-a-nominal-iv-and-a-continuous-dv-variable/124618#124618
r_squared = summary(lm(x ~ y))$r.squared
data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")
}else if(is_nominal(x) && is_numeric(y)){
r_squared = summary(lm(y ~x))$r.squared
data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")
}else {
warning(paste("unmatched column type combination: ", class(x), class(y)))
}
# finally add complete obs number and ratio to table
result %>% mutate(complete_obs_pairs=sum(!is.na(x) & !is.na(y)), complete_obs_ratio=complete_obs_pairs/length(x)) %>% rename(x=xName, y=yName)
}
# apply function to each variable combination
map2_df(df_comb$X1, df_comb$X2, f)
}
#correlation
df_res = mixed_assoc(seeds)
library(plyr)
#secondary function
mixed_assoc = function(df, cor_method="spearman", adjust_cramersv_bias=TRUE){
df_comb = expand.grid(names(df), names(df),  stringsAsFactors = F) %>% set_names("X1", "X2")
is_nominal = function(x) class(x) %in% c("factor", "character")
# https://community.rstudio.com/t/why-is-purr-is-numeric-deprecated/3559
# https://github.com/r-lib/rlang/issues/781
is_numeric <- function(x) { is.integer(x) || is_double(x)}
f = function(xName,yName) {
x =  pull(df, xName)
y =  pull(df, yName)
result = if(is_nominal(x) && is_nominal(y)){
# use bias corrected cramersV as described in https://rdrr.io/cran/rcompanion/man/cramerV.html
cv = cramerV(as.character(x), as.character(y), bias.correct = adjust_cramersv_bias)
data.frame(xName, yName, assoc=cv, type="cramersV")
}else if(is_numeric(x) && is_numeric(y)){
correlation = cor(x, y, method=cor_method, use="complete.obs")
data.frame(xName, yName, assoc=correlation, type="correlation")
}else if(is_numeric(x) && is_nominal(y)){
# from https://stats.stackexchange.com/questions/119835/correlation-between-a-nominal-iv-and-a-continuous-dv-variable/124618#124618
r_squared = summary(lm(x ~ y))$r.squared
data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")
}else if(is_nominal(x) && is_numeric(y)){
r_squared = summary(lm(y ~x))$r.squared
data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")
}else {
warning(paste("unmatched column type combination: ", class(x), class(y)))
}
# finally add complete obs number and ratio to table
result %>% mutate(complete_obs_pairs=sum(!is.na(x) & !is.na(y)), complete_obs_ratio=complete_obs_pairs/length(x)) %>% rename(x=xName, y=yName)
}
# apply function to each variable combination
map2_df(df_comb$X1, df_comb$X2, f)
}
#correlation
df_res = mixed_assoc(seeds)
install.packages("corrplot")
library(tidyverse)
library(dplyr)
library(rpart)
library(naniar)
library(FactoMineR)
library(fitdistrplus)
library(Hmisc)
library(MASS)
library(rcompanion)
library(lsr)
require(corrr)
library(stats)
library(e1071)
library(GGally)
ggpairs(seeds, aes(colour = Species, alpha = 0.4))
library(caret)
library(missMDA)
library(fastDummies)
library(mice)
library(reshape2)
library(ggplot2)
library(devtools)
library(ggbiplot)
library(fBasics)
library(corrplot)
library(tidyverse)
library(dplyr)
library(rpart)
library(naniar)
library(FactoMineR)
library(fitdistrplus)
library(Hmisc)
library(MASS)
library(rcompanion)
library(lsr)
require(corrr)
library(stats)
library(e1071)
library(GGally)
ggpairs(seeds, aes(colour = Species, alpha = 0.4))
library(caret)
library(missMDA)
library(fastDummies)
library(mice)
library(reshape2)
library(ggplot2)
library(devtools)
#correlation
df_res = mixed_assoc(seeds)
library(tidyverse)
library(dplyr)
library(rpart)
library(naniar)
library(FactoMineR)
library(fitdistrplus)
library(Hmisc)
library(MASS)
library(rcompanion)
library(lsr)
require(corrr)
library(stats)
library(e1071)
library(GGally)
ggpairs(seeds, aes(colour = Species, alpha = 0.4))
library(caret)
library(missMDA)
library(fastDummies)
library(mice)
library(reshape2)
library(ggplot2)
library(devtools)
library(ggbiplot)
library(fBasics)
library(corrplot)
library(numbers)
install.packages(c("mice","GGally","fastDummies","missMDA", "caret", "corrr", "dplyr", "e1071", "FactoMineR", "fitdistrplus", "Hmisc", "lsr", "naniar", "rcompanion", "tidyverse", "devtools", "fBasics"))
install.packages(c("mice", "GGally", "fastDummies", "missMDA", "caret", "corrr", "dplyr", "e1071", "FactoMineR", "fitdistrplus", "Hmisc", "lsr", "naniar", "rcompanion", "tidyverse", "devtools", "fBasics"))
install.packages(c("mice", "GGally", "fastDummies", "missMDA", "caret", "corrr", "dplyr", "e1071", "FactoMineR", "fitdistrplus", "Hmisc", "lsr", "naniar", "rcompanion", "tidyverse", "devtools", "fBasics"))
install.packages(c("mice","GGally","fastDummies","missMDA", "caret", "corrr", "dplyr", "e1071", "FactoMineR", "fitdistrplus", "Hmisc", "lsr", "naniar", "rcompanion", "tidyverse", "devtools", "fBasics"))
seeds
install.packages("e1071")
library(e1071)
head(train)
#Building the model
svmfit <- svm(type ~ area + perimeter + compactness + length + width + asymmetry + length_groove, data=train, scale=TRUE, kernel = "polinomial")
#Building the model
svmfit <- svm(type ~ area + perimeter + compactness + length + width + asymmetry + length_groove, data=train, scale=TRUE, kernel = "polynomial")
plot(svmfit)
svmfit
#Building the model
svmfit <- svm(type ~ area + perimeter + compactness + length + width + asymmetry + length_groove, data=train, scale=TRUE, kernel = "polynomial", type="class")
#Building the model
svmfit <- svm(type ~ area + perimeter + compactness + length + width + asymmetry + length_groove, data=train, scale=TRUE, kernel = "polynomial")
pred_svm <- predict(dtsvm,test) #predictions
pred_svm <- predict(svmfit,test) #predictions
mtab<-table(pred_svm, test[,8])
confusionMatrix(mtab)
library(tidyverse)
library(dplyr)
library(rpart)
library(naniar)
library(FactoMineR)
library(fitdistrplus)
library(Hmisc)
library(MASS)
library(rcompanion)
library(lsr)
require(corrr)
library(stats)
library(e1071)
library(GGally)
ggpairs(seeds, aes(colour = Species, alpha = 0.4))
library(caret)
library(missMDA)
library(fastDummies)
library(mice)
library(reshape2)
library(ggplot2)
library(devtools)
library(ggbiplot)
library(fBasics)
library(corrplot)
library(numbers)
library(e1071)
confusionMatrix(mtab)
svmfit.r <- svm(type ~ area + perimeter + compactness + length + width + asymmetry + length_groove, data=train, scale=TRUE, kernel = "radial")
pred_svm.r <- predict(svmfit.r,test)
mtab.r<-table(pred_svm.r, test[,8])
confusionMatrix(mtab.r)
##PCA (Pedro)----------------------------------------------------------------------------------
seeds.pca <- prcomp(seeds[,1:7], center = TRUE, scale = TRUE)
seeds.pca_df <- as.data.frame(seeds.pca$x[,c(1,2,3)]) #Dataset with only the first 3 principal components
##SVM with PCA
#Train/Test Split
df = seeds.pca_df
smp_size <- floor(0.75 * nrow(df))
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test <- df[-train_ind, ]
~
)
#Building the model
svmfit <- svm(type ~ area + perimeter + compactness + length + width + asymmetry + length_groove, data=train, scale=TRUE, kernel = "polynomial") #Polynomial kernel
head(train)
seeds.pca_df$type <- seeds$type
df = seeds.pca_df
smp_size <- floor(0.75 * nrow(df))
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test <- df[-train_ind, ]
#Building the model
svmfit <- svm(type ~ PC1 + PC2 + PC3, data=train, scale=TRUE, kernel = "polynomial") #Polynomial kernel
svmfit.r <- svm(type ~ PC1 + PC2 + PC3, data=train, scale=TRUE, kernel = "radial") #Radial kernel
#Evaluation
pred_svm <- predict(svmfit,test)
mtab<-table(pred_svm, test[,4])
confusionMatrix(mtab)
pred_svm.r <- predict(svmfit.r,test)
mtab.r<-table(pred_svm.r, test[,4])
confusionMatrix(mtab.r)
df = seeds
smp_size <- floor(0.75 * nrow(df))
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test <- df[-train_ind, ]
#Building the model
dtfit <- rpart(type ~ area + perimeter + compactness + length + width + asymmetry + length_groove, method="class", data=train)
pred_dt <- predict(dtfit, test, type="class") #predictions
mtab<-table(pred_dt, test[,8])
confusionMatrix(mtab)
plotcp(dtfit) #plot of the cross validation step to choose the complexity parameter (cp)
summary(dtfit)
plot(dtfit, uniform=TRUE,
main="DT without pca data")
plot(dtfit, uniform=TRUE,
main="DT without pca data")
text(dtfit, use.n=TRUE, all=TRUE, cex=.8)
df.pca = seeds.pca_df
smp_size <- floor(0.75 * nrow(df.pca))
train_ind <- sample(seq_len(nrow(df.pca)), size = smp_size)
train <- df.pca[train_ind, ]
test <- df.pca[-train_ind, ]
#Building the model
dtfit.pca <- rpart(type ~ PC1 + PC2 + PC3, method="class", data=train)
plotcp(dtfit.pca) #plot of the cross validation step to choose the complexity parameter (cp)
summary(dtfit.pca)
plot(dtfit.pca, uniform=TRUE,
main="DT with pca data")
text(dtfit.pca, use.n=TRUE, all=TRUE, cex=.8)
plot(dtfit, uniform=TRUE,
main="DT without pca data")
text(dtfit, use.n=TRUE, all=TRUE, cex=.8) #plo
plot(dtfit, uniform=TRUE,
main="DT without pca data")
text(dtfit, use.n=TRUE, all=TRUE, cex=.8)
#Building the model
dtfit <- rpart(type ~ area + perimeter + compactness + length + width + asymmetry + length_groove, method="entropy", data=train)
df = seeds
smp_size <- floor(0.75 * nrow(df))
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test <- df[-train_ind, ]
#Building the model
dtfit <- rpart(type ~ area + perimeter + compactness + length + width + asymmetry + length_groove, method="entropy", data=train)
#Building the model
dtfit <- rpart(type ~ area + perimeter + compactness + length + width + asymmetry + length_groove, method="gini", data=train)
#Building the model
dtfit <- rpart(type ~ area + perimeter + compactness + length + width + asymmetry + length_groove, method="class", data=train)
pred_dt <- predict(dtfit, test, type="class") #predictions
mtab<-table(pred_dt, test[,8])
confusionMatrix(mtab)
#Building the model
dtfit <- rpart(type ~ area + perimeter + compactness + length + width + asymmetry + length_groove, method="class", data=train)
pred_dt <- predict(dtfit, test, type="class") #predictions
mtab<-table(pred_dt, test[,8])
confusionMatrix(mtab)
df.pca = seeds.pca_df
smp_size <- floor(0.75 * nrow(df.pca))
train_ind <- sample(seq_len(nrow(df.pca)), size = smp_size)
train <- df.pca[train_ind, ]
test <- df.pca[-train_ind, ]
#Building the model
dtfit.pca <- rpart(type ~ PC1 + PC2 + PC3, method="class", data=train)
pred_dt.pca <- predict(dtfit.pca,test,type="class") #predictions
mtab<-table(pred_dt.pca, test[,4])
confusionMatrix(mtab)
pred_dt <- predict(dtfit, test, type="class") #predictions
mtab<-table(pred_dt, test[,8])
confusionMatrix(mtab)
df = seeds
smp_size <- floor(0.75 * nrow(df))
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test <- df[-train_ind, ]
#Building the model
dtfit <- rpart(type ~ area + perimeter + compactness + length + width + asymmetry + length_groove, method="class", data=train)
pred_dt <- predict(dtfit, test, type="class") #predictions
mtab<-table(pred_dt, test[,8])
confusionMatrix(mtab)
df.pca = seeds.pca_df
smp_size <- floor(0.75 * nrow(df.pca))
train_ind <- sample(seq_len(nrow(df.pca)), size = smp_size)
train <- df.pca[train_ind, ]
test <- df.pca[-train_ind, ]
#Building the model
dtfit.pca <- rpart(type ~ PC1 + PC2 + PC3, method="class", data=train)
pred_dt.pca <- predict(dtfit.pca,test,type="class") #predictions
mtab<-table(pred_dt.pca, test[,4])
confusionMatrix(mtab)
plot(svmfit)
df = seeds
smp_size <- floor(0.75 * nrow(df))
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test <- df[-train_ind, ]
#Building the model
svmfit <- svm(type ~ area + perimeter + compactness + length + width + asymmetry + length_groove, data=train, scale=TRUE, kernel = "polynomial") #Polynomial kernel
plot(svmfit)
plot.svmfit
plot(svmfit, train)
plot(svmfit, train, type ~ area + perimeter + compactness + length + width + asymmetry + length_groove)
plot(svmfit, train, type ~.)
summary(svmfit)
plot.svm(svmfit, train, type ~.)
mean(train)
mean(train$perimeter
)
mean(train$compactness)
mean(train$length)
mean(train$width)
mean(train$asymetry)
train$asymetry
mean(train$asymmetry)
mean(train$length_groove)
plot(svmfit, train, type ~ area,
slice = list(perimeter=15, compactness=1, length = 6, width= 3, asymmetry= 4, length_groove=5))
str(train)
plot(svmfit, train, as.numeric(type) ~ area,
slice = list(perimeter=15, compactness=1, length = 6, width= 3, asymmetry= 4, length_groove=5))
plot(svmfit, train, as.numeric(type) ~ area)
plot(svmfit, train, as.numeric(type) ~ area,
slice = list(perimeter=15, compactness=1, length = 6, width= 3, asymmetry= 4, length_groove=5))
mean(train$area)
plot(svmfit, train, as.numeric(type) ~ perimeter,
slice = list(area=15, compactness=1, length = 6, width= 3, asymmetry= 4, length_groove=5))
summary(svmfit)
svmfit.r <- svm(type ~ area + perimeter + compactness + length + width + asymmetry + length_groove, data=train, scale=TRUE, kernel = "radial") #Radial kernel
plot(svmfit.r, train, as.numeric(type) ~ perimeter,
slice = list(area=15, compactness=1, length = 6, width= 3, asymmetry= 4, length_groove=5))
plot(svmfit.r, train, as.numeric(type) ~ area,
slice = list(perimeter=15, compactness=1, length = 6, width= 3, asymmetry= 4, length_groove=5))
plot(svmfit, train, as.numeric(type) ~ perimeter,
slice = list(area=15, compactness=1, length = 6, width= 3, asymmetry= 4, length_groove=5))
df = seeds.pca_df
smp_size <- floor(0.75 * nrow(df))
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test <- df[-train_ind, ]
#Building the model
svmfit <- svm(type ~ PC1 + PC2 + PC3, data=train, scale=TRUE, kernel = "polynomial") #Polynomial kernel
svmfit.r <- svm(type ~ PC1 + PC2 + PC3, data=train, scale=TRUE, kernel = "radial") #Radial kernel
plot(svmfit, train)
plot(svmfit, train, as.numeric(type) ~.)
plot(svmfit, train, as.numeric(type) ~ area)
train
mean(train$PC1)
mean(train$PC2)
plot(svmfit, train, as.numeric(type) ~ PC3,
slice = list(PC1 = 0.06, PC2=0.03))
plot(svmfit.r, train, as.numeric(type) ~ PC3,
slice = list(PC1 = 0.06, PC2=0.03))
plot(svmfit, train)
#Building the model
svmfit <- svm(type ~ PC1 + PC2 + PC3, data=train, scale=TRUE, kernel = "polynomial") #Polynomial kernel
plot(svmfit, train)
plot(svmfit, train, type ~ PC1 + PC2 + PC3)
plot(svmfit, train, as.numeric(type) ~ PC1 + PC2 + PC3)
train$type <- as.numeric(train$type)
#Building the model
svmfit <- svm(type ~ PC1 + PC2 + PC3, data=train, scale=TRUE, kernel = "polynomial") #Polynomial kernel
plot(svmfit, train, type ~ PC1 + PC2 + PC3)
svmfit.r <- svm(type ~ PC1 + PC2 + PC3, data=train, scale=TRUE, kernel = "radial") #Radial kernel
plot(svmfit.r, train, type ~ PC1 + PC2 + PC3)
plot(svmfit.r, train, type ~ PC1 + PC2 + PC3)
plot(svmfit.r, train, type ~ PC1 + PC2 + PC3)
plot(svmfit.r, train, type ~ PC1 + PC2 + PC3)
plot.svm(svmfit.r, train, type ~ PC1 + PC2 + PC3)
plot(svmfit.r, train, type ~ PC1 + PC2 + PC3)
plot(svmfit. train, type ~ PC1 + PC2 + PC3)
plot(svmfit, train, type ~ PC1 + PC2 + PC3)
install.packages("kernlab")
library(kernlab)
model.ksvm = ksvm(type ~ area + perimeter + compactness + length + width + asymmetry + length_groove data = train, type="C-svc")
model.ksvm = ksvm(type ~ area + perimeter + compactness + length + width + asymmetry + length_groove, data = train, type="C-svc")
df = seeds
smp_size <- floor(0.75 * nrow(df))
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test <- df[-train_ind, ]
model.ksvm = ksvm(type ~ area + perimeter + compactness + length + width + asymmetry + length_groove, data = train, type="C-svc")
plot(model.ksvm, data=d)
plot(model.ksvm, data=train)
install.packages("gmim.r")
install.packages("gmum.r")
df = seeds.pca_df
smp_size <- floor(0.75 * nrow(df))
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test <- df[-train_ind, ]
#Building the model
svmfit <- svm(type ~ PC1 + PC2 + PC3, data=train, scale=TRUE, kernel = "polynomial") #Polynomial kernel
plot(svmfit)
plot(svmfit, data=test)
plot(svmfit, data=train[,-4])
plot(svmfit, data=train[])
plot(svmfit, data=train, type ~ PC1 + PC2 + PC3)
train$type <- as.numeric(train$type)
#Building the model
svmfit <- svm(type ~ PC1 + PC2 + PC3, data=train, scale=TRUE, kernel = "polynomial") #Polynomial kernel
plot(svmfit, data=train, type ~ PC1 + PC2 + PC3)
plot(svmfit, data=train, type ~ PC1)
plot(svmfit, data=train, type ~ PC1 + PC2 + PC3)
df = seeds.pca_df
smp_size <- floor(0.75 * nrow(df))
train_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[train_ind, ]
test <- df[-train_ind, ]
#Building the model
svmfit <- svm(type ~ PC1 + PC2 + PC3, data=train, scale=TRUE, kernel = "polynomial") #Polynomial kernel
svmfit.r <- svm(type ~ PC1 + PC2 + PC3, data=train, scale=TRUE, kernel = "radial") #Radial kernel
plot(svmfit, data=train, type ~ PC1 + PC2 + PC3)
summary(svmfit)
summary(svmfit.r)
svmfit.r <- svm(type ~ PC1 + PC2 + PC3, data=train, scale=TRUE, kernel = "radial", method="C-classification") #Radial kernel
plot(svmfit, data=train, type ~ PC1 + PC2 + PC3)
plot(svmfit, train, type ~ PC1,
slice=list(PC2, PC3))
train$type <- as.numeric(train$type)
#Building the model
svmfit <- svm(type ~ PC1 + PC2 + PC3, data=train, scale=TRUE, kernel = "polynomial") #Polynomial kernel
svmfit.r <- svm(type ~ PC1 + PC2 + PC3, data=train, scale=TRUE, kernel = "radial", method="C-classification") #Radial kernel
plot(svmfit, train, type ~ PC1,
slice=list(PC2, PC3))
